{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11 Lab - Model Evaluation\n",
    "\n",
    "## Directions\n",
    "\n",
    "\n",
    "The due dates for each are indicated in the Syllabus and the course calendar. If anything is unclear, please email EN685.648@gmail.com the official email for the course or ask questions in the Lab discussion area on Blackboard.\n",
    "\n",
    "The Labs also present technical material that augments the lectures and \"book\".  You should read through the entire lab at the start of each module.\n",
    "\n",
    "<div style=\"background: mistyrose; color: firebrick; border: 2px solid darkred; padding: 5px; margin: 10px;\">\n",
    "Please follow the directions and make sure you provide the requested output. Failure to do so may result in a lower grade even if the code is correct or even 0 points.\n",
    "</div>\n",
    "\n",
    "1. Show all work/steps/calculations using Code and Markdown cells.\n",
    "2. Submit your notebook (.ipynb).\n",
    "3. You may use any core Python libraries or Numpy/Scipy. **Additionally, code from the Module notebooks and lectures is fair to use and modify.** You may also consult Stackoverflow (SO). If you use something from SO, please place a comment with the URL to document the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import patsy\n",
    "import sklearn.linear_model as linear\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# load whatever other libraries you need including models.py\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Improvement\n",
    "\n",
    "As we saw in both the Linear Regression and Logistic Regression modules, there is a Statistician's view of Model Evaluation (and perhaps, Improvement) and a Machine Learning view of Model Evaluation and Improvement.\n",
    "\n",
    "We'll be working with the **insurance data**.\n",
    "\n",
    "**1. Load the data, perform your transformations, and using the Bootstrap version of the Linear Regression function, estimate your final model from Lab 10 and show the Bootstrap results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model produced at the end of the Lab 10 solution:\n",
    "\n",
    "\"charges ~ age_sq + male + bmi + smoke_yes + smoke_yes:bmi + smoke_yes:bmi_above_30 + children\"\n",
    "\n",
    "Let's load up the data, and create the transformations listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>age_sq</th>\n",
       "      <th>smoke_yes</th>\n",
       "      <th>male</th>\n",
       "      <th>bmi_above_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>361</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>1089</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>10600.54830</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>2205.98080</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1629.83350</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>female</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "      <td>2007.94500</td>\n",
       "      <td>441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>female</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "      <td>29141.36030</td>\n",
       "      <td>3721</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex     bmi  children smoker     region      charges  age_sq  \\\n",
       "0      19  female  27.900         0    yes  southwest  16884.92400     361   \n",
       "1      18    male  33.770         1     no  southeast   1725.55230     324   \n",
       "2      28    male  33.000         3     no  southeast   4449.46200     784   \n",
       "3      33    male  22.705         0     no  northwest  21984.47061    1089   \n",
       "4      32    male  28.880         0     no  northwest   3866.85520    1024   \n",
       "...   ...     ...     ...       ...    ...        ...          ...     ...   \n",
       "1333   50    male  30.970         3     no  northwest  10600.54830    2500   \n",
       "1334   18  female  31.920         0     no  northeast   2205.98080     324   \n",
       "1335   18  female  36.850         0     no  southeast   1629.83350     324   \n",
       "1336   21  female  25.800         0     no  southwest   2007.94500     441   \n",
       "1337   61  female  29.070         0    yes  northwest  29141.36030    3721   \n",
       "\n",
       "      smoke_yes  male  bmi_above_30  \n",
       "0             1     0             0  \n",
       "1             0     1             1  \n",
       "2             0     1             1  \n",
       "3             0     1             0  \n",
       "4             0     1             0  \n",
       "...         ...   ...           ...  \n",
       "1333          0     1             1  \n",
       "1334          0     0             1  \n",
       "1335          0     0             1  \n",
       "1336          0     0             0  \n",
       "1337          1     0             0  \n",
       "\n",
       "[1338 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"insurance.csv\")\n",
    "\n",
    "data[\"age_sq\"]       = data.age * data.age\n",
    "data[\"smoke_yes\"]    = pd.get_dummies(data.smoker, prefix=\"smoke\").smoke_yes\n",
    "data[\"male\"]         = pd.get_dummies(data.sex).male\n",
    "data[\"bmi_above_30\"] = (data.bmi > 30).map({True: 1, False: 0})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <table>\n",
       "    <tr><th> model </th><td> charges ~ age_sq + male + bmi + smoke_yes + smoke_yes:bmi + smoke_yes:bmi_above_30 + children </td></tr>\n",
       "    <tr><th colspan=2>coefficients</th><th>95% BCI</th></tr>\n",
       "    <tr><th> $\\beta_0$ </th><td> 2014.88 </td><td>(649.57, 3384.89)</td></tr><tr><td>  age_sq  ($\\beta_1$) </td><td> 3.34 </td><td>(3.18, 3.59)</tr><tr><td>  male  ($\\beta_2$) </td><td> -516.74 </td><td>(-947.35, -103.20)</tr><tr><td>  bmi  ($\\beta_3$) </td><td> 3.56 </td><td>(-47.00, 40.80)</tr><tr><td>  smoke_yes  ($\\beta_4$) </td><td> 1632.98 </td><td>(-1831.47, 4879.07)</tr><tr><td>  smoke_yes:bmi  ($\\beta_5$) </td><td> 464.32 </td><td>(336.08, 585.46)</tr><tr><td>  smoke_yes:bmi_above_30  ($\\beta_6$) </td><td> 15225.57 </td><td>(13552.55, 16543.21)</tr><tr><td>  children ($\\beta_7$) </td><td> 651.47 </td><td>(473.92, 832.64)</tr><tr><th colspan=2>metrics</th><th>95% BCI</th></tr><tr><th> $\\sigma$ </th><td> 4378.69 </td><td>(3920.05, 4764.85) </td></tr><tr><th> $R^2$ </th><td> 0.87 </td><td>(0.85, 0.89)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"charges ~ age_sq + male + bmi + smoke_yes + smoke_yes:bmi + smoke_yes:bmi_above_30 + children\"\n",
    "final = models.bootstrap_linear_regression(model, data=data)\n",
    "models.describe_bootstrap_lr(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are indeed pretty good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Perform three rounds of 10-fold cross validation, estimating $R^2$ and $\\sigma$ each round. Using the results for the test data, calculate 95% Bootstrap estimates of the credible intervals for each.** Comment on these intervals and the intervals from above. Are the average values different? Are the intervals different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we'll use sklearn's `cross_val_score`. However, first we need to get our model into a form that sklearn can take in and evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices(model, data, return_type=\"matrix\")\n",
    "reg = linear.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88200657, 0.89374155, 0.86889218, 0.85295221, 0.94917338,\n",
       "       0.86017784, 0.84655537, 0.77446399, 0.83321713, 0.87561323,\n",
       "       0.79893548, 0.94441111, 0.81031815, 0.92460551, 0.90966819,\n",
       "       0.86769227, 0.84683167, 0.836579  , 0.90736098, 0.85525215,\n",
       "       0.92950911, 0.94280255, 0.87651198, 0.92745154, 0.83359276,\n",
       "       0.79143157, 0.84175431, 0.74746667, 0.85269734, 0.88589089])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 42)\n",
    "R2s = cross_val_score(estimator = reg, X = X, y = y, cv = cv)\n",
    "R2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `RepeatedKFold` did the 10 splits 3 times.\n",
    "\n",
    "sklearn's `LinearRegression` uses $R^2$ as its scoring method, so the numbers we see above are the various $R^2$ values for each fold. But we need both $R^2$ and $\\sigma$, so let's define our own scoring function that calculates $\\sigma$ and combine those scores with our $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sigma(model, X, y):\n",
    "    result = {}\n",
    "    coefficients = model.coef_[0]\n",
    "\n",
    "    y_hat = model.predict(X)\n",
    "    residuals = y - y_hat\n",
    "    \n",
    "    sum_squared_error = sum(e ** 2 for e in residuals)[0]\n",
    "\n",
    "    n = len(residuals)\n",
    "    k = len(coefficients)\n",
    "\n",
    "    sigma = np.sqrt(sum_squared_error / (n - k))\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 42)\n",
    "sigmas = cross_val_score(estimator = reg, X = X, y = y, cv = cv, scoring = calc_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have results for both (and both started with the same random seed so they give the same splits), we can carry out bootstrapping to get our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data, f, n=100):\n",
    "    m = len(data)\n",
    "    return np.array(\n",
    "        [f(np.random.choice(data, len(data), replace=True)) for _ in range(n)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_posterior    = bootstrap_sample(data = R2s   , f = np.mean, n = 500)\n",
    "sigma_posterior = bootstrap_sample(data = sigmas, f = np.mean, n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8471807 , 0.88556733])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mstats.mquantiles(R2_posterior, prob = [0.025, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 95% BCI for the $R^2$ of our model is 0.847 to 0.886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4163.39668023, 4735.3879571 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mstats.mquantiles(sigma_posterior, prob = [0.025, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 95% BCI for the $\\sigma$ of our model is 4163 to 4735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Using Learning Curves and $\\sigma$ determine if more data will improve the estimation of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def chunk(xs, n):\n",
    "    k, m = divmod(len(xs), n)\n",
    "    return [xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "def mse( errors):\n",
    "    n = len( errors)\n",
    "    squared_error = np.sum( [e**2 for e in errors])\n",
    "    return np.sqrt((1.0/n) * squared_error)\n",
    "\n",
    "def data_collection():\n",
    "    result = dict()\n",
    "    result[ \"train\"] = defaultdict( list)\n",
    "    result[ \"test\"] = defaultdict( list)\n",
    "    return result\n",
    "\n",
    "def learning_curves(algorithm, formula, data, evaluate, fold_count=10, repetitions=3, increment=1):\n",
    "    indices = list(range(len(data)))\n",
    "    results = data_collection()\n",
    "    for _ in range(repetitions):\n",
    "        random.shuffle(indices)\n",
    "        folds = chunk(indices, fold_count)\n",
    "        for fold in folds:\n",
    "            test_data = data.iloc[ fold]\n",
    "            train_indices = [idx for idx in indices if idx not in fold]\n",
    "            train_data = data.iloc[train_indices]\n",
    "            for i in list(range(increment, 100, increment)) + [100]:\n",
    "                # ensures 100% is always picked.\n",
    "                # the indices are already shuffled so we only need to take ever\n",
    "                # increasing chunks\n",
    "                train_chunk_size = int( np.ceil((i/100)*len( train_indices)))\n",
    "                train_data_chunk = data.iloc[train_indices[0:train_chunk_size]]\n",
    "                # we calculate the model\n",
    "                result = algorithm(formula, data=train_data_chunk)\n",
    "                model = result[\"model\"]\n",
    "                # we calculate the results for the training data subset\n",
    "                y, X = patsy.dmatrices( formula, train_data_chunk, return_type=\"matrix\")\n",
    "                result = models.summarize(formula, X, y, model)\n",
    "                metric = evaluate(result)\n",
    "                results[\"train\"][i].append( metric)\n",
    "                # we calculate the results for the test data.\n",
    "                y, X = patsy.dmatrices( formula, test_data, return_type=\"matrix\")\n",
    "                result = models.summarize(formula, X, y, model)\n",
    "                metric = evaluate(result)\n",
    "                results[\"test\"][i].append( metric)\n",
    "    # process results\n",
    "    # Rely on the CLT...\n",
    "    statistics = {}\n",
    "    for k, v in results[\"train\"].items():\n",
    "        statistics[ k] = (np.mean(v), np.std(v))\n",
    "    results[\"train\"] = statistics\n",
    "    statistics = {}\n",
    "    for k, v in results[\"test\"].items():\n",
    "        statistics[ k] = (np.mean(v), np.std(v))\n",
    "    results[\"test\"] = statistics\n",
    "    return results\n",
    "\n",
    "def sse(results):\n",
    "    errors = results['residuals']\n",
    "    n = len(errors)\n",
    "    squared_error = sum(e**2 for e in errors)\n",
    "    return np.sqrt((1.0/n)*squared_error)\n",
    "\n",
    "def results_to_curves( curve, results):\n",
    "    all_statistics = results[ curve]\n",
    "    keys = list( all_statistics.keys())\n",
    "    keys.sort()\n",
    "    mean = []\n",
    "    upper = []\n",
    "    lower = []\n",
    "    for k in keys:\n",
    "        m, s = all_statistics[ k]\n",
    "        mean.append( m)\n",
    "        upper.append( m + 2*s)\n",
    "        lower.append( m - 2*s)\n",
    "    return keys, lower, mean, upper\n",
    "\n",
    "def plot_learning_curves( results, metric, desired=None, zoom=False, credible=True):\n",
    "    figure = plt.figure(figsize=(10,6))\n",
    "    \n",
    "    axes = figure.add_subplot(1, 1, 1)\n",
    "    \n",
    "    xs, train_lower, train_mean, train_upper = results_to_curves( \"train\", results)\n",
    "    _, test_lower, test_mean, test_upper = results_to_curves( \"test\", results)\n",
    "    \n",
    "    axes.plot( xs, train_mean, color=\"steelblue\", label=\"train\")\n",
    "    axes.plot( xs, test_mean, color=\"firebrick\", label=\"test\")\n",
    "    if credible:\n",
    "        axes.fill_between( xs, train_upper, train_lower, color=\"steelblue\", alpha=0.25)\n",
    "        axes.fill_between( xs, test_upper, test_lower, color=\"firebrick\", alpha=0.25)\n",
    "        \n",
    "    if desired:\n",
    "        if type(desired) is tuple:\n",
    "            axes.axhline((desired[0] + desired[1])/2.0, color=\"gold\", label=\"desired\")\n",
    "            axes.fill_between( xs, desired[1], desired[0], color=\"gold\", alpha=0.25)\n",
    "        else:\n",
    "            axes.axhline( desired, color=\"gold\", label=\"desired\")\n",
    "    \n",
    "    axes.legend()\n",
    "    axes.set_xlabel( \"training set (%)\")\n",
    "    axes.set_ylabel( metric)\n",
    "    axes.set_title(\"Learning Curves\")\n",
    "    \n",
    "    if zoom:\n",
    "        y_lower = int( 0.9 * np.amin([train_lower[-1], test_lower[-1]]))\n",
    "        y_upper = int( 1.1 * np.amax([train_upper[-1], test_upper[-1]]))\n",
    "        axes.set_ylim((y_lower, y_upper))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5ccc0034420f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-88ec2531e67b>\u001b[0m in \u001b[0;36mlearning_curves\u001b[0;34m(algorithm, formula, data, evaluate, fold_count, repetitions, increment)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# we calculate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# we calculate the results for the training data subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatsy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdmatrices\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "results = learning_curves(models.linear_regression, model, data, sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, <module 'sklearn.linear_model' from '/home/natemcintosh/miniconda3/envs/en685648/lib/python3.8/site-packages/sklearn/linear_model/__init__.py'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-cb5e819ba309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Insurance Learning Curve\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-548f5cc76e63>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(estimator, title, X, y, axes, ylim, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n\u001b[0m\u001b[1;32m     74\u001b[0m                        \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                        return_times=True)\n",
      "\u001b[0;32m~/miniconda3/envs/en685648/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/en685648/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0mcv_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0mn_max_training_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/en685648/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/en685648/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \"\"\"\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0m\u001b[1;32m    403\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, <module 'sklearn.linear_model' from '/home/natemcintosh/miniconda3/envs/en685648/lib/python3.8/site-packages/sklearn/linear_model/__init__.py'> was passed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(estimator=linear, title=\"Insurance Learning Curve\", X=X, y=y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. It was shown that `age_sq` improved the performance of the model. Perhaps a different polynomial would have been better. Generate Validation Curves for `age` = [1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5] and select the best transformation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Using Ridge Regression to estimate a model for the insurance data. Compare it with your final Linear Regression model.** (If you get far ahead, you may need to write your own function. Here are the sklearn docs: http://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
